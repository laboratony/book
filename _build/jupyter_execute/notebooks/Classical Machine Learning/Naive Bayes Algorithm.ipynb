{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a717c075",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aac2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14015bc",
   "metadata": {},
   "source": [
    "Naive Bayes Algorithm คือ classication ที่ใช้ Bayes' theorem ทำได้โดยใช้ `sklearn.naive_bayes.MultinomialNB\n",
    "`\n",
    "\n",
    "<img src=\"../images/bayes_theorem.png\" width=\"400\" /><br />\n",
    "\n",
    "ตัวอย่างเช่น การระบุว่าอีเมลหนึ่งเป็นอีเมลปกติ (N) หรือ spam (S)\n",
    "\n",
    "เริ่มจากนับทุกคำในทุกอีเมลที่ปกติ แล้วคำนวณ probability ที่คำหนึ่งจะปรากฏในอีเมลปกติ ($P(\\text{word}|N)$) และนับทุกคำในทุกอีเมล spam แล้วคำนวณ probability ที่คำหนึ่งจะปรากฏในอีเมล spam ($P(\\text{word}|S)$)\n",
    "\n",
    "<img src=\"../images/normal_spam_emails.png\" width=\"900\" /><br />\n",
    "\n",
    "จากภาพ คำนวณ Prior (probability ที่อีเมลจะเป็นอีเมลปกติ) จะได้\n",
    "\n",
    "$$P(N)=\\frac{8}{8+4}=0.67$$\n",
    "\n",
    "$$P(S)=1-P(N)=0.33$$\n",
    "\n",
    "และคำนวณ Posterior probability ที่อีเมลจะเป็นอีเมลปกติ\n",
    "\n",
    "เช่น ถ้าอีเมลปรากฏข้อความ \"Dear Friend\" เราจะสามารถคำนวณ\n",
    "\n",
    "$$P(N\\:|\\:\\text{Dear Friend}) = P(N) \\times P(\\text{Dear}\\:|\\:N) \\times P(\\text{Friend}\\:|\\:N) = 0.09$$\n",
    "\n",
    "$$P(S\\:|\\:\\text{Dear Friend}) = P(S) \\times P(\\text{Dear}\\:|\\:S) \\times P(\\text{Friend}\\:|\\:S) = 0.01$$\n",
    "\n",
    "เช่น ถ้าอีเมลปรากฏข้อความ \"Lunch Money Money Money Money\" เราจะสามารถคำนวณ\n",
    "\n",
    "$$P(N\\:|\\:\\text{Lunch Money Money Money Money}) = P(N) \\times P(\\text{Lunch}\\:|\\:N) \\times P(\\text{Money}\\:|\\:N)^4 = 0.000002$$\n",
    "\n",
    "$$P(S\\:|\\:\\text{Lunch Money Money Money Money}) = P(S) \\times P(\\text{Lunch}\\:|\\:S) \\times P(\\text{Money}\\:|\\:S)^4 = 0.00$$\n",
    "\n",
    "$P(S\\:|\\:\\text{Lunch Money Money Money Money}) = 0.00$ ซึ่งไม่น่าจะถูกต้องในความเป็นจริง เพราะอีเมล spam น่าจะขอเงิน\n",
    "    \n",
    "เพื่อป้องกันการคำนวณ probability แล้วได้ 0 เราต้องทำ <b>smoothing</b> ซึ่งเป็นการเพิ่มจำนวนในแต่ละ feature (มักจะ +1) เพื่อไม่ให้มี feature ใดมีจำนวนเป็น 0\n",
    "\n",
    "<img src=\"../images/smoothing.png\" width=\"900\" /><br />\n",
    "\n",
    "$$P(N\\:|\\:\\text{Lunch Money Money Money Money}) = P(N) \\times P(\\text{Lunch}\\:|\\:N) \\times P(\\text{Money}\\:|\\:N)^4 = 0.00001$$\n",
    "\n",
    "$$P(S\\:|\\:\\text{Lunch Money Money Money Money}) = P(S) \\times P(\\text{Lunch}\\:|\\:S) \\times P(\\text{Money}\\:|\\:S)^4 = 0.00122$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581024fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @7im: Aaaand @ScottWalker just eliminated a...</td>\n",
       "      <td>814547316258512896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @annelongfield: Know any budding young clim...</td>\n",
       "      <td>955607502699290625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>@SenSanders Mainly because climate change has ...</td>\n",
       "      <td>959276999603716101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @StephenSchlegel: she's thinking about how ...</td>\n",
       "      <td>798860441870970883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>willingly sacrificing common courtesy in order...</td>\n",
       "      <td>953371679799070721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8784</th>\n",
       "      <td>0</td>\n",
       "      <td>@BarackObama @capitalweather Gary Johnson: For...</td>\n",
       "      <td>779353621427138560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8785</th>\n",
       "      <td>1</td>\n",
       "      <td>So fucking mad, climate change isn't something...</td>\n",
       "      <td>796529677829623808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8786</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @moklick: NASA created a page about climate...</td>\n",
       "      <td>842032360124227584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8787</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SenSanders: We have a president-elect who ...</td>\n",
       "      <td>797897893243822080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8788</th>\n",
       "      <td>0</td>\n",
       "      <td>@KMOV Man, this global warming is really getti...</td>\n",
       "      <td>955793233879162881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8789 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                            message  \\\n",
       "0             1  RT @7im: Aaaand @ScottWalker just eliminated a...   \n",
       "1             0  RT @annelongfield: Know any budding young clim...   \n",
       "2            -1  @SenSanders Mainly because climate change has ...   \n",
       "3             1  RT @StephenSchlegel: she's thinking about how ...   \n",
       "4             1  willingly sacrificing common courtesy in order...   \n",
       "...         ...                                                ...   \n",
       "8784          0  @BarackObama @capitalweather Gary Johnson: For...   \n",
       "8785          1  So fucking mad, climate change isn't something...   \n",
       "8786          0  RT @moklick: NASA created a page about climate...   \n",
       "8787          1  RT @SenSanders: We have a president-elect who ...   \n",
       "8788          0  @KMOV Man, this global warming is really getti...   \n",
       "\n",
       "                 tweetid  \n",
       "0     814547316258512896  \n",
       "1     955607502699290625  \n",
       "2     959276999603716101  \n",
       "3     798860441870970883  \n",
       "4     953371679799070721  \n",
       "...                  ...  \n",
       "8784  779353621427138560  \n",
       "8785  796529677829623808  \n",
       "8786  842032360124227584  \n",
       "8787  797897893243822080  \n",
       "8788  955793233879162881  \n",
       "\n",
       "[8789 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/twitter_sentiment_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed1f32fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates and NAs\n",
    "data = data.drop_duplicates().dropna()\n",
    "# Feature (X) and target (y)\n",
    "X = data[\"message\"]\n",
    "y = data[\"sentiment\"]\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35715abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_data(X):\n",
    "    X_preprocessed = X.copy()\n",
    "    for idx, txt in X_preprocessed.items():\n",
    "        txt = txt.lower()  # lower\n",
    "        txt = ''.join(word for word in txt if not word.isdigit())  # remove numbers\n",
    "        for punctuation in string.punctuation:\n",
    "            txt = txt.replace(punctuation, '')  # remove punctuation\n",
    "        word_tokens = word_tokenize(txt)  # tokenize\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = [w for w in word_tokens if not w in stop_words]  # remove stopwords\n",
    "        word_tokens = [WordNetLemmatizer().lemmatize(word) for word in word_tokens]  # lemmatize\n",
    "        final_text = ' '.join(word_tokens)\n",
    "        X_preprocessed.loc[idx] = final_text\n",
    "    return X_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e019c26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform text pre-processing\n",
    "X_train_preprocessed = preprocess_text_data(X_train)\n",
    "X_test_preprocessed = preprocess_text_data(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa6b35ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorise training and test sets\n",
    "vectorizer = TfidfVectorizer(max_features=15).fit(X_train_preprocessed)\n",
    "X_train_vectorised = vectorizer.transform(X_train_preprocessed)\n",
    "X_test_vectorised = vectorizer.transform(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6d8a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create naive bayes model\n",
    "nb = MultinomialNB().fit(X_train_vectorised, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a93cd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2193\n",
       "0     370\n",
       "2      71\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the prediction on test texts\n",
    "pd.Series(nb.predict(X_test_vectorised)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd713729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5432801822323462"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute accuracy score\n",
    "nb.score(X_test_vectorised, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}